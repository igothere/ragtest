# rag.py

import os
import sys
import psycopg2
import traceback
import re # ì •ê·œ í‘œí˜„ì‹ì„ ìœ„í•´ ì¶”ê°€
from sentence_transformers import SentenceTransformer
from pdfminer.high_level import extract_text
import requests
from typing import List, Tuple
import json

# ... (ì„¤ì • ë¶€ë¶„ì€ ë™ì¼) ...
TARGET_DIR = "./docs"
MODEL_NAME = "nlpai-lab/KURE-v1"
DB_CONFIG = {
    "host": "localhost", "port": "5432", "dbname": "ragtest",
    "user": "eden", "password": "qwer123"
}
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100

# LLM ì„¤ì • (OLLAMA ì„¤ì •)
OLLAMA_ENDPOINT = os.getenv("OLLAMA_ENDPOINT", "https://api.hamonize.com/ollama/api/chat")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "airun-chat:latest")

# -------------------------------------------------
# í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
# -------------------------------------------------
def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”ë¥¼ ìœ„í•œ í•¨ìˆ˜"""
    if not text:
        return ""
    
    # 0. NULL ë°”ì´íŠ¸ ë° ì œì–´ ë¬¸ì ì œê±° (PostgreSQL ì˜¤ë¥˜ ë°©ì§€)
    text = text.replace('\x00', '')
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    # 1. ì†Œë¬¸ìë¡œ ë³€í™˜
    text = text.lower()
    
    # 2. ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì  ì œì™¸)
    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ê·¸ë¦¬ê³  ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•œ ì˜¨ì (.)ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    # text = re.sub(r'[^ê°€-í£a-z0-9\s\.]', ' ', text)
    text = re.sub(r'[^ê°€-í£a-z0-9\s\./_-]', ' ', text)
    
    # 3. ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¶•ì†Œ
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 4. (ì„ íƒì‚¬í•­) ë¶ˆìš©ì–´(Stopword) ì œê±°
    # ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•Šê±°ë‚˜, ì˜¤íˆë ¤ ë¬¸ë§¥ì„ í•´ì¹  ìˆ˜ ìˆì–´ ìµœê·¼ì—ëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë„ í•©ë‹ˆë‹¤.
    # í•„ìš” ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³ , ë¶ˆìš©ì–´ ì‚¬ì „ì„ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
    # stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ê³¼', 'ì™€', 'ë„']
    # words = text.split()
    # words = [word for word in words if word not in stopwords]
    # text = ' '.join(words)
    
    return text

# -------------------------------------------------
# LLM ê¸°ë°˜ ìš”ì•½ ë° ì²­í‚¹ í•¨ìˆ˜ë“¤
# -------------------------------------------------

def call_ollama_api(messages: List[dict], max_tokens: int = 150) -> str:
    """OLLAMA APIë¥¼ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        payload = {
            "model": OLLAMA_MODEL,
            "messages": messages,
            "stream": False,
            "options": {
                "num_predict": max_tokens,
                "temperature": 0.3
            }
        }
        
        response = requests.post(
            OLLAMA_ENDPOINT,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            return result.get("message", {}).get("content", "").strip()
        else:
            print(f"âš ï¸ OLLAMA API ì˜¤ë¥˜: {response.status_code} - {response.text}")
            return ""
            
    except Exception as e:
        print(f"âš ï¸ OLLAMA API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""

def get_paragraph_summary(text: str, max_tokens: int = 150) -> str:
    """ë‹¨ë½ì˜ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    if not OLLAMA_ENDPOINT or not OLLAMA_MODEL:
        print("âš ï¸ OLLAMA ì„¤ì •ì´ ë˜ì§€ ì•Šì•„ ìš”ì•½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""
    
    messages = [
        {
            "role": "system", 
            "content": "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
        },
        {
            "role": "user", 
            "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{text[:2000]}"  # í† í° ì œí•œì„ ìœ„í•´ 2000ìë¡œ ì œí•œ
        }
    ]
    
    return call_ollama_api(messages, max_tokens)

def find_topic_boundaries(text: str, paragraph_size: int = 1000) -> List[Tuple[int, str]]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì œ ì „í™˜ì ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    if not OLLAMA_ENDPOINT or not OLLAMA_MODEL:
        print("âš ï¸ OLLAMA ì„¤ì •ì´ ë˜ì§€ ì•Šì•„ ì£¼ì œ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return [(0, "ì „ì²´ ë¬¸ì„œ")]
    
    # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ë½ ë‹¨ìœ„ë¡œ ë¶„í• 
    paragraphs = []
    for i in range(0, len(text), paragraph_size):
        paragraph = text[i:i + paragraph_size]
        if paragraph.strip():
            paragraphs.append((i, paragraph))
    
    if len(paragraphs) <= 1:
        return [(0, "ì „ì²´ ë¬¸ì„œ")]
    
    boundaries = [(0, "ë¬¸ì„œ ì‹œì‘")]
    
    try:
        # ê° ë‹¨ë½ì˜ ìš”ì•½ ìƒì„±
        summaries = []
        for i, (pos, paragraph) in enumerate(paragraphs):
            print(f"  ğŸ“ ë‹¨ë½ {i+1}/{len(paragraphs)} ìš”ì•½ ìƒì„± ì¤‘...")
            summary = get_paragraph_summary(paragraph)
            summaries.append((pos, summary))
        
        # ì—°ì†ëœ ìš”ì•½ë“¤ì„ ë¹„êµí•˜ì—¬ ì£¼ì œ ì „í™˜ì  ì°¾ê¸°
        for i in range(1, len(summaries)):
            prev_summary = summaries[i-1][1]
            curr_summary = summaries[i][1]
            
            if prev_summary and curr_summary:
                # LLMì—ê²Œ ë‘ ìš”ì•½ì´ ë‹¤ë¥¸ ì£¼ì œì¸ì§€ íŒë‹¨ ìš”ì²­
                is_different_topic = check_topic_change(prev_summary, curr_summary)
                if is_different_topic:
                    boundaries.append((summaries[i][0], f"ì£¼ì œ ì „í™˜: {curr_summary[:50]}..."))
                    print(f"  ğŸ”„ ì£¼ì œ ì „í™˜ì  ë°œê²¬: ìœ„ì¹˜ {summaries[i][0]}")
        
    except Exception as e:
        print(f"âš ï¸ ì£¼ì œ ê²½ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return [(0, "ì „ì²´ ë¬¸ì„œ")]
    
    return boundaries

def check_topic_change(prev_summary: str, curr_summary: str) -> bool:
    """ë‘ ìš”ì•½ ì‚¬ì´ì— ì£¼ì œ ë³€í™”ê°€ ìˆëŠ”ì§€ LLMìœ¼ë¡œ íŒë‹¨"""
    messages = [
        {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‘ í…ìŠ¤íŠ¸ ìš”ì•½ì´ ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”. 'YES' ë˜ëŠ” 'NO'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
        },
        {
            "role": "user",
            "content": f"ë‹¤ìŒ ë‘ ìš”ì•½ì´ ì„œë¡œ ë‹¤ë¥¸ ì£¼ì œë¥¼ ë‹¤ë£¨ë‚˜ìš”?\n\nì´ì „ ìš”ì•½: {prev_summary}\n\ní˜„ì¬ ìš”ì•½: {curr_summary}\n\në‹¤ë¥¸ ì£¼ì œë¼ë©´ YES, ê°™ì€ ì£¼ì œë¼ë©´ NOë¡œ ë‹µë³€í•˜ì„¸ìš”."
        }
    ]
    
    try:
        answer = call_ollama_api(messages, max_tokens=10)
        return answer.upper() == "YES"
    except Exception as e:
        print(f"âš ï¸ ì£¼ì œ ë³€í™” íŒë‹¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def summarization_aware_chunking(text: str, min_chunk_size: int = 300, max_chunk_size: int = 800) -> List[str]:
    """ìš”ì•½ ê¸°ë°˜ ì²­í‚¹ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ§  LLM ê¸°ë°˜ ìš”ì•½ ì²­í‚¹ ì‹œì‘...")
    
    if not OLLAMA_ENDPOINT or not OLLAMA_MODEL:
        print("âš ï¸ OLLAMA ì„¤ì •ì´ ì—†ì–´ ê¸°ë³¸ ì²­í‚¹ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)
    
    # ì£¼ì œ ê²½ê³„ì  ì°¾ê¸°
    boundaries = find_topic_boundaries(text)
    print(f"  ğŸ“ {len(boundaries)}ê°œì˜ ì£¼ì œ ê²½ê³„ì  ë°œê²¬")
    
    chunks = []
    
    for i in range(len(boundaries)):
        start_pos = boundaries[i][0]
        end_pos = boundaries[i + 1][0] if i + 1 < len(boundaries) else len(text)
        
        section_text = text[start_pos:end_pos].strip()
        if not section_text:
            continue
        
        # ì„¹ì…˜ì´ ë„ˆë¬´ í¬ë©´ ì¶”ê°€ë¡œ ë¶„í• 
        if len(section_text) <= max_chunk_size:
            chunks.append(section_text)
            print(f"  âœ‚ï¸ ì²­í¬ ìƒì„±: {len(section_text)}ì (ì£¼ì œ: {boundaries[i][1][:30]}...)")
        else:
            # í° ì„¹ì…˜ì€ ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì¶”ê°€ ë¶„í• 
            sub_chunks = chunk_text(section_text, max_chunk_size, CHUNK_OVERLAP)
            chunks.extend(sub_chunks)
            print(f"  âœ‚ï¸ ëŒ€í˜• ì„¹ì…˜ì„ {len(sub_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
    
    # ë„ˆë¬´ ì‘ì€ ì²­í¬ë“¤ì€ ì¸ì ‘ ì²­í¬ì™€ ë³‘í•©
    merged_chunks = []
    current_chunk = ""
    
    for chunk in chunks:
        if len(current_chunk + chunk) <= max_chunk_size:
            current_chunk += (" " + chunk if current_chunk else chunk)
        else:
            if current_chunk:
                merged_chunks.append(current_chunk)
            current_chunk = chunk
    
    if current_chunk:
        merged_chunks.append(current_chunk)
    
    print(f"  ğŸ¯ ìµœì¢… {len(merged_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
    return merged_chunks

# -------------------------------------------------

# ... (chunk_text í•¨ìˆ˜ëŠ” ë™ì¼) ...
def chunk_text(text, chunk_size, overlap):
    if not text: return []
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks

# DB ì‚½ì… í•¨ìˆ˜ ìˆ˜ì •: original_filename ì¸ì ì¶”ê°€
def insert_chunk_to_db(cursor, title, content, embedding, source_file, original_filename, chunk_index, total_chunks):
    # ëª¨ë“  ë¬¸ìì—´ í•„ë“œì—ì„œ NULL ë°”ì´íŠ¸ ì œê±°
    def clean_string(s):
        if s is None:
            return None
        return str(s).replace('\x00', '').replace('\r', '').replace('\n\n\n', '\n\n')
    
    title = clean_string(title)
    content = clean_string(content)
    source_file = clean_string(source_file)
    original_filename = clean_string(original_filename)
    
    embedding_str = "[" + ",".join(map(str, embedding)) + "]"
    sql = """
        INSERT INTO documents (title, content, embedding, source_file, original_filename, chunk_index, total_chunks)
        VALUES (%s, %s, %s::vector, %s, %s, %s, %s)
    """
    try:
        print(f"    ğŸ’¾ DB ì‚½ì… ì‹œë„: ì²­í¬ {chunk_index}/{total_chunks}")
        cursor.execute(sql, (title, content, embedding_str, source_file, original_filename, chunk_index, total_chunks))
        print(f"    âœ… DB ì‚½ì… ì„±ê³µ")
    except Exception as e:
        print(f"    âŒ DB ì‚½ì… ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"    ë””ë²„ê·¸ ì •ë³´: title ê¸¸ì´={len(title) if title else 0}, content ê¸¸ì´={len(content) if content else 0}")
        raise

# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ ìˆ˜ì •: original_filename ì¸ì ì¶”ê°€
def process_file(file_path, unique_filename, original_filename, model, cursor):
    """ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ ë‹¨ìœ„ë¡œ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\nğŸ“„ '{original_filename}' (ì €ì¥ëª…: {unique_filename}) íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
    try:
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‚´ìš© ì¶”ì¶œ
        ext = os.path.splitext(unique_filename)[1].lower()
        text = ""
        if ext == '.pdf':
            text = extract_text(file_path)
        elif ext in ['.txt', '.md']:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
        else:
            print(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {unique_filename}")
            return False

        if not text or not text.strip():
            print(f"  âš ï¸ ë‚´ìš©ì´ ì—†ëŠ” íŒŒì¼ì…ë‹ˆë‹¤: {original_filename}")
            return True

        # --- ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ---
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ê¸° ì „ì— ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        print("  - í…ìŠ¤íŠ¸ ì •ê·œí™” ìˆ˜í–‰ ì¤‘...")
        normalized_text = normalize_text(text)
        print(f"  - ì •ê·œí™” ì™„ë£Œ (ì›ë³¸: {len(text)}ì -> ì •ê·œí™”: {len(normalized_text)}ì)")
        
        # ìš”ì•½ ê¸°ë°˜ ì²­í‚¹ ë˜ëŠ” ê¸°ë³¸ ì²­í‚¹ ì„ íƒ
        use_summarization_chunking = os.getenv("USE_SUMMARIZATION_CHUNKING", "false").lower() == "true"
        
        if use_summarization_chunking and OLLAMA_ENDPOINT and OLLAMA_MODEL:
            print("  ğŸ§  OLLAMA ê¸°ë°˜ ìš”ì•½ ì²­í‚¹ ì‚¬ìš©")
            chunks = summarization_aware_chunking(normalized_text)
        else:
            print("  âœ‚ï¸ ê¸°ë³¸ ì²­í‚¹ ë°©ì‹ ì‚¬ìš©")
            chunks = chunk_text(normalized_text, CHUNK_SIZE, CHUNK_OVERLAP)
        
        total_chunks = len(chunks)
        print(f"  âœ… {total_chunks}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
        # ---------------------------

        # ê° ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  DBì— ì €ì¥
        for i, chunk in enumerate(chunks):
            # ì²­í¬ ìì²´ëŠ” ì´ë¯¸ ì •ê·œí™”ë˜ì—ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ì„ë² ë”©
            if not chunk: continue # ë¹ˆ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
            
            chunk_index = i + 1
            print(f"  ğŸ”„ ì²­í¬ {chunk_index}/{total_chunks} ì²˜ë¦¬ ì¤‘...")
            
            embedding = model.encode(chunk).tolist()
            
            chunk_title = f"{original_filename}_chunk_{chunk_index}"
            insert_chunk_to_db(cursor, chunk_title, chunk, embedding, unique_filename, original_filename, chunk_index, total_chunks)

        print(f"  âœ… íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ: {original_filename}")
        return True

    except Exception as e:
        print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {original_filename} - {e}")
        traceback.print_exc()
        return False

def main():
    print("==============================================")
    print("ğŸš€ RAG ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰")
    print("==============================================")
    
    # ì¸ì 2ê°œë¥¼ ë°›ë„ë¡ ìˆ˜ì •
    if len(sys.argv) < 3:
        print("âŒ ì‚¬ìš©ë²•: python rag.py <ê³ ìœ  íŒŒì¼ëª…> <ì›ë³¸ íŒŒì¼ëª…>")
        sys.exit(1)
    
    unique_filename = sys.argv[1]
    original_filename = sys.argv[2]
    print(f"ğŸ¯ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼: {original_filename} (ê³ ìœ ëª…: {unique_filename})")

    model = None
    conn = None
    try:
        print("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        model = SentenceTransformer(MODEL_NAME)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        print("ğŸ˜ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„...")
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")

        # íŒŒì¼ ê²½ë¡œëŠ” ê³ ìœ  íŒŒì¼ëª…ìœ¼ë¡œ í™•ì¸
        file_path = os.path.join(TARGET_DIR, unique_filename)
        print(f"ğŸ” íŒŒì¼ ê²½ë¡œ í™•ì¸: {file_path}")
        if not os.path.isfile(file_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            sys.exit(1)

        # process_file í˜¸ì¶œ ì‹œ ë‘ íŒŒì¼ëª… ëª¨ë‘ ì „ë‹¬
        if process_file(file_path, unique_filename, original_filename, model, cur):
            conn.commit()
            print(f"\nğŸ‰ ìµœì¢… ì»¤ë°‹ ì™„ë£Œ: '{original_filename}' ë°ì´í„°ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            conn.rollback()
            print(f"\nâš ï¸ ë¡¤ë°± ì™„ë£Œ: '{original_filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ DB ë³€ê²½ì‚¬í•­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(1)

    except Exception as e:
        print("\nâŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ!")
        traceback.print_exc()
        if conn:
            conn.rollback()
        sys.exit(1)
    finally:
        if conn:
            conn.close()
            print("\nğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("ğŸ ëª¨ë“  ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()