# rag.py

import os
import sys
import psycopg2
import traceback
import re # ì •ê·œ í‘œí˜„ì‹ì„ ìœ„í•´ ì¶”ê°€
from sentence_transformers import SentenceTransformer
from pdfminer.high_level import extract_text

# ... (ì„¤ì • ë¶€ë¶„ì€ ë™ì¼) ...
TARGET_DIR = "/home/eden/rag/docs"
MODEL_NAME = "nlpai-lab/KURE-v1"
DB_CONFIG = {
    "host": "localhost", "port": "5432", "dbname": "ragtest",
    "user": "eden", "password": "qwer123"
}
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100

def simulate_markdown_headers(text: str) -> str:
    """
    PDF ë¬¸ì„œì— ë§ˆí¬ë‹¤ìš´ í—¤ë” í˜•ì‹ì„ ê°€ì§œë¡œ ì¶”ê°€í•˜ì—¬ HLM ì²­í‚¹ì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦.
    - ë¹ˆ ì¤„ ë‹¤ìŒ ë¬¸ì¥ì€ # í—¤ë”ë¡œ ì²˜ë¦¬
    - ë˜ëŠ” 50ì ì´í•˜ì˜ ë‹¨ë… ì¤„ì„ í—¤ë”ë¡œ ê°„ì£¼
    """
    lines = text.splitlines()
    new_lines = []
    for i, line in enumerate(lines):
        stripped = line.strip()
        if not stripped:
            new_lines.append("")  # ë¹ˆ ì¤„ ìœ ì§€
            continue

        # ì¡°ê±´ 1: ì´ì „ ì¤„ì´ ë¹„ì–´ ìˆê³ , í˜„ì¬ ì¤„ì´ ì§§ê³  ë¬¸ì¥ì²˜ëŸ¼ ë³´ì„
        is_possible_header = (
            i > 0 and not lines[i - 1].strip() and len(stripped) <= 50
        )

        # ì¡°ê±´ 2: í•œ ì¤„ì§œë¦¬ ì„¹ì…˜ ì œëª©ì²˜ëŸ¼ ìƒê¸´ ì¤„
        if is_possible_header or (len(stripped.split()) <= 8 and stripped.endswith(":") == False):
            new_lines.append(f"# {stripped}")  # ê°€ì§œ í—¤ë”
        else:
            new_lines.append(stripped)

    return "\n".join(new_lines)


def hlm_chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """
    ë§ˆí¬ë‹¤ìš´ ê¸°ë°˜ HLM ì²­í‚¹ í•¨ìˆ˜: í—¤ë”(# ~ ######) ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë‚˜ëˆˆ í›„ chunk_size ë‹¨ìœ„ë¡œ ìë¦„
    """
    pattern = re.compile(r'(#{1,6})\s+(.*)')  # ë§ˆí¬ë‹¤ìš´ í—¤ë”
    lines = text.splitlines()

    chunks = []
    current_title = "Untitled"
    current_buffer = ""

    for line in lines:
        match = pattern.match(line)
        if match:
            # í˜„ì¬ ë²„í¼ ì €ì¥
            if current_buffer.strip():
                subchunks = chunk_text(current_buffer, chunk_size, overlap)
                for idx, sub in enumerate(subchunks):
                    # titled = f"[{current_title}]\n{sub}"
                    # chunks.append(titled)
                    titled_chunk = f"[{current_title}]\n{sub}"
                    chunks.append(titled_chunk)
            current_title = match.group(2).strip()
            current_buffer = ""
        else:
            current_buffer += line + "\n"

    # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬
    if current_buffer.strip():
        subchunks = chunk_text(current_buffer, chunk_size, overlap)
        for idx, sub in enumerate(subchunks):
            # titled = f"[{current_title}]\n{sub}"
            # chunks.append(titled)
            titled_chunk = f"[{current_title}]\n{sub}"
            chunks.append(titled_chunk)

    print(f"âœ… HLM ì²­í‚¹ ì™„ë£Œ. ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨.")
    for i, c in enumerate(chunks[:5]):
        print(f"  â–¶ï¸ Chunk {i+1} Preview:\n{c[:100]}\n")

    return chunks

# -------------------------------------------------
# í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
# -------------------------------------------------
def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”ë¥¼ ìœ„í•œ í•¨ìˆ˜"""
    # 1. ì†Œë¬¸ìë¡œ ë³€í™˜
    text = text.lower()
    
    # 2. ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì  ì œì™¸)
    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ê·¸ë¦¬ê³  ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•œ ì˜¨ì (.)ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    # text = re.sub(r'[^ê°€-í£a-z0-9\s\.]', ' ', text)
    # text = re.sub(r'[^ê°€-í£a-z0-9\s\./_-]', ' ', text)

    text = re.sub(r'[^ê°€-í£a-z0-9\s\./_\-#]', ' ', text)
    
    # 3. ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¶•ì†Œ
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 4. (ì„ íƒì‚¬í•­) ë¶ˆìš©ì–´(Stopword) ì œê±°
    # ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•Šê±°ë‚˜, ì˜¤íˆë ¤ ë¬¸ë§¥ì„ í•´ì¹  ìˆ˜ ìˆì–´ ìµœê·¼ì—ëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë„ í•©ë‹ˆë‹¤.
    # í•„ìš” ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³ , ë¶ˆìš©ì–´ ì‚¬ì „ì„ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
    # stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ê³¼', 'ì™€', 'ë„']
    # words = text.split()
    # words = [word for word in words if word not in stopwords]
    # text = ' '.join(words)
    
    return text
# -------------------------------------------------

# ... (chunk_text í•¨ìˆ˜ëŠ” ë™ì¼) ...
def chunk_text(text, chunk_size, overlap):
    if not text: return []
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks

# DB ì‚½ì… í•¨ìˆ˜ ìˆ˜ì •: original_filename ì¸ì ì¶”ê°€
def insert_chunk_to_db(cursor, title, content, embedding, source_file, original_filename, chunk_index, total_chunks):
    embedding_str = "[" + ",".join(map(str, embedding)) + "]"
    sql = """
        INSERT INTO documents (title, content, embedding, source_file, original_filename, chunk_index, total_chunks)
        VALUES (%s, %s, %s::vector, %s, %s, %s, %s)
    """
    try:
        print(f"    ğŸ’¾ DB ì‚½ì… ì‹œë„: ì²­í¬ {chunk_index}/{total_chunks}")
        cursor.execute(sql, (title, content, embedding_str, source_file, original_filename, chunk_index, total_chunks))
        print(f"    âœ… DB ì‚½ì… ì„±ê³µ")
    except Exception as e:
        print(f"    âŒ DB ì‚½ì… ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ ìˆ˜ì •: original_filename ì¸ì ì¶”ê°€
def process_file(file_path, unique_filename, original_filename, model, cursor):
    """ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ ë‹¨ìœ„ë¡œ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\nğŸ“„ '{original_filename}' (ì €ì¥ëª…: {unique_filename}) íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
    try:
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‚´ìš© ì¶”ì¶œ
        ext = os.path.splitext(unique_filename)[1].lower()
        text = ""
        if ext == '.pdf':
            text = extract_text(file_path)
            # PDFì—ëŠ” í—¤ë”ê°€ ì—†ìœ¼ë¯€ë¡œ, ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¥¼ ì‹œë®¬ë ˆì´ì…˜
            print("  - PDFìš© ê°€ì§œ í—¤ë” ìƒì„± ì¤‘ (simulate_markdown_headers)...")
            normalized_text = normalize_text(text)
            normalized_text = simulate_markdown_headers(normalized_text)
        elif ext in ['.txt', '.md']:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            normalized_text = normalize_text(text)
        else:
            print(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {unique_filename}")
            return False

        if not text or not text.strip():
            print(f"  âš ï¸ ë‚´ìš©ì´ ì—†ëŠ” íŒŒì¼ì…ë‹ˆë‹¤: {original_filename}")
            return True

        # --- ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ---
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ê¸° ì „ì— ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        print("  - í…ìŠ¤íŠ¸ ì •ê·œí™” ìˆ˜í–‰ ì¤‘...")
        
        print(f"  - ì •ê·œí™” ì™„ë£Œ (ì›ë³¸: {len(text)}ì -> ì •ê·œí™”: {len(normalized_text)}ì)")
        # ---------------------------

        # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹
        # chunks = chunk_text(normalized_text, CHUNK_SIZE, CHUNK_OVERLAP)
        chunks = hlm_chunk_text(normalized_text, CHUNK_SIZE, CHUNK_OVERLAP)
        total_chunks = len(chunks)
        print(f"  âœ‚ï¸ {total_chunks}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

        # ê° ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  DBì— ì €ì¥
        for i, chunk in enumerate(chunks):
            # ì²­í¬ ìì²´ëŠ” ì´ë¯¸ ì •ê·œí™”ë˜ì—ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ì„ë² ë”©
            if not chunk: continue # ë¹ˆ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
            
            chunk_index = i + 1
            print(f"  ğŸ”„ ì²­í¬ {chunk_index}/{total_chunks} ì²˜ë¦¬ ì¤‘...")
            
            embedding = model.encode(chunk).tolist()

            first_line = chunk.split("\n")[0].strip()
            if first_line.startswith("[") and first_line.endswith("]"):
                chunk_title = first_line.strip("[]")
            else:
                chunk_title = "Untitled"
            
            # chunk_title = f"{original_filename}_chunk_{chunk_index}"
            # chunk_title = chunk.split("\n")[0].strip("[]")  # "[ì œëª©]" â†’ "ì œëª©"
            insert_chunk_to_db(cursor, chunk_title, chunk, embedding, unique_filename, original_filename, chunk_index, total_chunks)

        print(f"  âœ… íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ: {original_filename}")
        return True

    except Exception as e:
        print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {original_filename} - {e}")
        traceback.print_exc()
        return False

def main():
    print("==============================================")
    print("ğŸš€ RAG ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰")
    print("==============================================")
    
    # ì¸ì 2ê°œë¥¼ ë°›ë„ë¡ ìˆ˜ì •
    if len(sys.argv) < 3:
        print("âŒ ì‚¬ìš©ë²•: python rag.py <ê³ ìœ  íŒŒì¼ëª…> <ì›ë³¸ íŒŒì¼ëª…>")
        sys.exit(1)
    
    unique_filename = sys.argv[1]
    original_filename = sys.argv[2]
    print(f"ğŸ¯ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼: {original_filename} (ê³ ìœ ëª…: {unique_filename})")

    model = None
    conn = None
    try:
        print("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        model = SentenceTransformer(MODEL_NAME)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        print("ğŸ˜ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„...")
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")

        # íŒŒì¼ ê²½ë¡œëŠ” ê³ ìœ  íŒŒì¼ëª…ìœ¼ë¡œ í™•ì¸
        file_path = os.path.join(TARGET_DIR, unique_filename)
        print(f"ğŸ” íŒŒì¼ ê²½ë¡œ í™•ì¸: {file_path}")
        if not os.path.isfile(file_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            sys.exit(1)

        # process_file í˜¸ì¶œ ì‹œ ë‘ íŒŒì¼ëª… ëª¨ë‘ ì „ë‹¬
        if process_file(file_path, unique_filename, original_filename, model, cur):
            conn.commit()
            print(f"\nğŸ‰ ìµœì¢… ì»¤ë°‹ ì™„ë£Œ: '{original_filename}' ë°ì´í„°ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            conn.rollback()
            print(f"\nâš ï¸ ë¡¤ë°± ì™„ë£Œ: '{original_filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ DB ë³€ê²½ì‚¬í•­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(1)

    except Exception as e:
        print("\nâŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ!")
        traceback.print_exc()
        if conn:
            conn.rollback()
        sys.exit(1)
    finally:
        if conn:
            conn.close()
            print("\nğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("ğŸ ëª¨ë“  ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()