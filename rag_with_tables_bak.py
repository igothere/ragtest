#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í‘œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ëŠ” RAG ì‹œìŠ¤í…œ

ê¸°ì¡´ rag.pyë¥¼ í™•ì¥í•˜ì—¬ PDF í‘œë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì²˜ë¦¬
"""

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import sys
import psycopg2
import traceback
import re
import requests
import json
import pandas as pd
import pdfplumber
from typing import List, Tuple, Dict, Any
from model_manager import ModelManager, get_model_with_fallback

# ì„¤ì •
TARGET_DIR = "./docs"
MODEL_NAME = "nlpai-lab/KURE-v1"
DB_CONFIG = {
    "host": "localhost", "port": "5432", "dbname": "ragtest",
    "user": "eden", "password": "qwer123"
}
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100

# OLLAMA ì„¤ì •
OLLAMA_ENDPOINT = os.getenv("OLLAMA_ENDPOINT", "https://api.hamonize.com/ollama/api/chat")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "airun-chat:latest")

def extract_pdf_with_tables(file_path: str) -> List[Dict[str, Any]]:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ + í‘œë¥¼ ë¬¶ì–´ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì¶”ì¶œ"""
    results = []

    try:
        with pdfplumber.open(file_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                print(f"  ğŸ“„ í˜ì´ì§€ {page_num} ë¶„ì„ ì¤‘...")

                tables = page.extract_tables()
                markdown_tables = []
                searchable_table_texts = []

                if tables:
                    for table_idx, table in enumerate(tables):
                        if table and len(table) > 1:
                            try:
                                df = pd.DataFrame(table[1:], columns=table[0])
                                df = df.dropna(how='all').dropna(axis=1, how='all')
                                df = df.applymap(lambda x: str(x).replace('\x00', '') if pd.notna(x) else x)

                                if not df.empty:
                                    markdown = df.to_markdown(index=False).replace('\x00', '')
                                    markdown_tables.append(f"[í‘œ {table_idx + 1}]\n{markdown}")
                                    searchable_table_texts.append(create_table_searchable_text(df))
                                    print(f"    âœ… í‘œ ë°œê²¬: {len(df)}í–‰ Ã— {len(df.columns)}ì—´")
                            except Exception as e:
                                print(f"    âš ï¸ í‘œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

                text = page.extract_text()
                clean_text = ''
                if text and text.strip():
                    clean_text = text.replace('\x00', '')
                    clean_text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', clean_text).strip()
                    print(f"    ğŸ“ í…ìŠ¤íŠ¸: {len(clean_text)}ì")

                # í…ìŠ¤íŠ¸ + í‘œ í•©ì¹˜ê¸°
                full_content = ''
                embedding_text = ''

                if clean_text:
                    full_content += clean_text + "\n\n"
                    embedding_text += clean_text + "\n\n"
                if markdown_tables:
                    full_content += "\n\n".join(markdown_tables)
                    embedding_text += "\n\n".join(searchable_table_texts)

                if full_content.strip():
                    results.append({
                        'type': 'mixed',
                        'title': f'í˜ì´ì§€ {page_num} í…ìŠ¤íŠ¸+í‘œ',
                        'content': full_content.strip(),
                        'embedding_text': embedding_text.strip(),
                        'metadata': {
                            'page': page_num,
                            'table_count': len(markdown_tables),
                            'has_tables': bool(markdown_tables)
                        }
                    })

    except Exception as e:
        print(f"  âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        from pdfminer.high_level import extract_text
        text = extract_text(file_path)
        if text:
            results.append({
                'type': 'text',
                'title': 'í´ë°± í…ìŠ¤íŠ¸',
                'content': text,
                'embedding_text': text,
                'metadata': {'fallback': True}
            })

    return results

# def extract_pdf_with_tables(file_path: str) -> List[Dict[str, Any]]:
#     """PDFì—ì„œ í‘œì™€ í…ìŠ¤íŠ¸ë¥¼ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œ"""
#     results = []
    
#     try:
#         with pdfplumber.open(file_path) as pdf:
#             for page_num, page in enumerate(pdf.pages, 1):
#                 print(f"  ğŸ“„ í˜ì´ì§€ {page_num} ë¶„ì„ ì¤‘...")
                
#                 # í‘œ ì¶”ì¶œ
#                 tables = page.extract_tables()
                
#                 if tables:
#                     for table_idx, table in enumerate(tables):
#                         if table and len(table) > 1:
#                             try:
#                                 # í‘œë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
#                                 df = pd.DataFrame(table[1:], columns=table[0])
#                                 df = df.dropna(how='all').dropna(axis=1, how='all')
                                
#                                 # DataFrameì˜ ëª¨ë“  ê°’ì—ì„œ NULL ë°”ì´íŠ¸ ì œê±°
#                                 df = df.applymap(lambda x: str(x).replace('\x00', '') if pd.notna(x) else x)
                                
#                                 if not df.empty:
#                                     # ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹œì—ë„ NULL ë°”ì´íŠ¸ ì œê±°
#                                     markdown_content = df.to_markdown(index=False).replace('\x00', '')
                                    
#                                     table_info = {
#                                         'type': 'table',
#                                         'page': page_num,
#                                         'table_index': table_idx + 1,
#                                         'content': markdown_content,
#                                         'searchable_text': create_table_searchable_text(df),
#                                         'metadata': {
#                                             'rows': len(df),
#                                             'columns': len(df.columns),
#                                             'column_names': [str(col).replace('\x00', '') for col in df.columns.tolist()]
#                                         }
#                                     }
#                                     results.append(table_info)
#                                     print(f"    âœ… í‘œ ë°œê²¬: {len(df)}í–‰ Ã— {len(df.columns)}ì—´")
#                             except Exception as e:
#                                 print(f"    âš ï¸ í‘œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
#                 # í…ìŠ¤íŠ¸ ì¶”ì¶œ (í‘œ ì˜ì—­ ì œì™¸ëŠ” ë³µì¡í•˜ë¯€ë¡œ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©)
#                 text = page.extract_text()
#                 if text and text.strip():
#                     # NULL ë°”ì´íŠ¸ ë° ì œì–´ ë¬¸ì ì œê±°
#                     clean_text = text.replace('\x00', '')
#                     clean_text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', clean_text)
#                     clean_text = clean_text.strip()
                    
#                     if clean_text:
#                         # í‘œê°€ ìˆëŠ” í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ëŠ” í‘œ ì •ë³´ì™€ í•¨ê»˜ ì €ì¥
#                         has_tables = len([t for t in tables if t]) > 0
#                         text_info = {
#                             'type': 'text',
#                             'page': page_num,
#                             'content': clean_text,
#                             'searchable_text': clean_text,
#                             'metadata': {
#                                 'has_tables': has_tables,
#                                 'table_count': len([t for t in tables if t])
#                             }
#                         }
#                         results.append(text_info)
#                         print(f"    ğŸ“ í…ìŠ¤íŠ¸: {len(clean_text)}ì")
    
#     except Exception as e:
#         print(f"  âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
#         # ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
#         from pdfminer.high_level import extract_text
#         text = extract_text(file_path)
#         if text:
#             results.append({
#                 'type': 'text',
#                 'page': 1,
#                 'content': text,
#                 'searchable_text': text,
#                 'metadata': {'fallback': True}
#             })
    
#     return results

def create_table_searchable_text(df: pd.DataFrame) -> str:
    """í‘œë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    # ì»¬ëŸ¼ëª… + ëª¨ë“  ì…€ ê°’ì„ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    column_text = "ì»¬ëŸ¼: " + ", ".join(str(col).replace('\x00', '') for col in df.columns.tolist())
    
    # ê° í–‰ì˜ ë°ì´í„°ë¥¼ ìì—°ì–´ í˜•íƒœë¡œ ë³€í™˜
    row_texts = []
    for _, row in df.iterrows():
        row_items = []
        for col, val in row.items():
            if pd.notna(val) and str(val).strip():
                # NULL ë°”ì´íŠ¸ ì œê±°
                clean_col = str(col).replace('\x00', '')
                clean_val = str(val).replace('\x00', '')
                row_items.append(f"{clean_col}: {clean_val}")
        if row_items:
            row_texts.append(" | ".join(row_items))
    
    result = f"[í‘œ ë°ì´í„°] {column_text}\n" + "\n".join(row_texts)
    # ìµœì¢…ì ìœ¼ë¡œ NULL ë°”ì´íŠ¸ ì œê±°
    return result.replace('\x00', '')

def create_structured_chunks(extracted_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë³€í™˜"""
    chunks = []
    
    for item in extracted_data:
        if item['type'] == 'table':
            # í‘œëŠ” í•˜ë‚˜ì˜ ì²­í¬ë¡œ (í¬ê¸°ì— ë”°ë¼ ë¶„í•  ê°€ëŠ¥)
            chunk = {
                'type': 'table',
                'title': f"í˜ì´ì§€ {item['page']} í‘œ {item['table_index']}",
                'content': f"[í‘œ {item['table_index']}]\n{item['content']}",
                'embedding_text': item['searchable_text'],
                'metadata': item['metadata']
            }
            chunks.append(chunk)
            
        elif item['type'] == 'text':
            # í…ìŠ¤íŠ¸ëŠ” ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²­í‚¹
            text_chunks = chunk_text(item['content'], CHUNK_SIZE, CHUNK_OVERLAP)
            for i, text_chunk in enumerate(text_chunks):
                chunk = {
                    'type': 'text',
                    'title': f"í˜ì´ì§€ {item['page']} í…ìŠ¤íŠ¸ {i+1}",
                    'content': text_chunk,
                    'embedding_text': text_chunk,
                    'metadata': {'page': item['page'], 'chunk_index': i+1}
                }
                chunks.append(chunk)
    
    return chunks

from transformers import AutoTokenizer

# ëª¨ë¸ í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained("nlpai-lab/KURE-v1")

def chunk_text(text, chunk_size=480, overlap=40):
    """KURE-v1 ê¸°ì¤€ í† í° ë‹¨ìœ„ ì²­í¬ ë¶„í• """
    if not text:
        return []

    input_ids = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    start = 0
    end = chunk_size

    while start < len(input_ids):
        chunk_ids = input_ids[start:end]
        chunk_text = tokenizer.decode(chunk_ids)
        chunks.append(chunk_text)

        # ë‹¤ìŒ ì²­í¬ë¡œ ì´ë™ (overlap ê³ ë ¤)
        start = end - overlap
        end = start + chunk_size

    return chunks


def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜ (NULL ë°”ì´íŠ¸ ì œê±° í¬í•¨)"""
    if not text:
        return ""
    
    # NULL ë°”ì´íŠ¸ ì œê±° (PostgreSQL ì˜¤ë¥˜ ë°©ì§€)
    text = text.replace('\x00', '')
    
    # ê¸°íƒ€ ì œì–´ ë¬¸ì ì œê±°
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    text = text.lower()
    text = re.sub(r'[^ê°€-í£a-z0-9\s\./_-]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def insert_chunk_to_db(cursor, title, content, embedding, source_file, original_filename, chunk_index, total_chunks, chunk_type='text', metadata=None):
    """DB ì‚½ì… í•¨ìˆ˜ (í‘œ ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    # ëª¨ë“  ë¬¸ìì—´ í•„ë“œì—ì„œ NULL ë°”ì´íŠ¸ ì œê±°
    def clean_string(s):
        if s is None:
            return None
        return str(s).replace('\x00', '').replace('\r', '').replace('\n\n\n', '\n\n')
    
    title = clean_string(title)
    content = clean_string(content)
    source_file = clean_string(source_file)
    original_filename = clean_string(original_filename)
    chunk_type = clean_string(chunk_type)
    
    embedding_str = "[" + ",".join(map(str, embedding)) + "]"
    metadata_json = json.dumps(metadata) if metadata else None
    if metadata_json:
        metadata_json = clean_string(metadata_json)
    
    sql = """
        INSERT INTO documents (title, content, embedding, source_file, original_filename, chunk_index, total_chunks, chunk_type, metadata)
        VALUES (%s, %s, %s::vector, %s, %s, %s, %s, %s, %s)
    """
    try:
        print(f"    ğŸ’¾ DB ì‚½ì… ì‹œë„: {chunk_type} ì²­í¬ {chunk_index}/{total_chunks}")
        cursor.execute(sql, (title, content, embedding_str, source_file, original_filename, chunk_index, total_chunks, chunk_type, metadata_json))
        print(f"    âœ… DB ì‚½ì… ì„±ê³µ")
    except Exception as e:
        print(f"    âŒ DB ì‚½ì… ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"    ë””ë²„ê·¸ ì •ë³´: title ê¸¸ì´={len(title) if title else 0}, content ê¸¸ì´={len(content) if content else 0}")
        raise

def process_file_with_tables(file_path, unique_filename, original_filename, model, cursor):
    """í‘œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ íŒŒì¼ ì²˜ë¦¬"""
    print(f"\nğŸ“„ '{original_filename}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ (í‘œ êµ¬ì¡° ìœ ì§€)...")
    
    try:
        # PDFì—ì„œ í‘œì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print("  ğŸ” PDFì—ì„œ í‘œì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        extracted_data = extract_pdf_with_tables(file_path)
        
        if not extracted_data:
            print(f"  âš ï¸ ì¶”ì¶œëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤: {original_filename}")
            return True
        
        # êµ¬ì¡°í™”ëœ ì²­í¬ ìƒì„±
        # print("  ğŸ§© êµ¬ì¡°í™”ëœ ì²­í¬ ìƒì„± ì¤‘...")
        # chunks = create_structured_chunks(extracted_data)
        # total_chunks = len(chunks)
        
        # print(f"  âœ‚ï¸ {total_chunks}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

         # êµ¬ì¡°í™”ëœ ì²­í¬ ìƒì„±
        print("  ğŸ§© í…ìŠ¤íŠ¸+í‘œ í†µí•© ì²­í¬ ìƒì„± ì¤‘...")
        chunks = extracted_data
        total_chunks = len(chunks)
        print(f"  âœ‚ï¸ {total_chunks}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
        
        # ê° ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  DBì— ì €ì¥
        for i, chunk in enumerate(chunks):
            if not chunk['embedding_text'].strip():
                continue
            
            chunk_index = i + 1
            print(f"  ğŸ”„ ì²­í¬ {chunk_index}/{total_chunks} ì²˜ë¦¬ ì¤‘... ({chunk['type']})")
            
            # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì •ê·œí™”
            normalized_text = normalize_text(chunk['embedding_text'])
            embedding = model.encode(normalized_text).tolist()
            
            insert_chunk_to_db(
                cursor, 
                chunk['title'], 
                chunk['content'], 
                embedding, 
                unique_filename, 
                original_filename, 
                chunk_index, 
                total_chunks,
                chunk['type'],
                chunk['metadata']
            )
        
        print(f"  âœ… íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ: {original_filename}")
        return True
        
    except Exception as e:
        print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {original_filename} - {e}")
        traceback.print_exc()
        return False

def main():
    print("==============================================")
    print("ğŸš€ RAG ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (í‘œ êµ¬ì¡° ìœ ì§€)")
    print("==============================================")
    
    if len(sys.argv) < 3:
        print("âŒ ì‚¬ìš©ë²•: python rag_with_tables.py <ê³ ìœ  íŒŒì¼ëª…> <ì›ë³¸ íŒŒì¼ëª…>")
        sys.exit(1)
    
    unique_filename = sys.argv[1]
    original_filename = sys.argv[2]
    print(f"ğŸ¯ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼: {original_filename} (ê³ ìœ ëª…: {unique_filename})")

    model = None
    conn = None
    try:
        print("ğŸ¤– ê³µìœ  ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        model = get_model_with_fallback()
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        print("ğŸ˜ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„...")
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")

        file_path = os.path.join(TARGET_DIR, unique_filename)
        print(f"ğŸ” íŒŒì¼ ê²½ë¡œ í™•ì¸: {file_path}")
        if not os.path.isfile(file_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            sys.exit(1)

        if process_file_with_tables(file_path, unique_filename, original_filename, model, cur):
            conn.commit()
            print(f"\nğŸ‰ ìµœì¢… ì»¤ë°‹ ì™„ë£Œ: '{original_filename}' ë°ì´í„°ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            conn.rollback()
            print(f"\nâš ï¸ ë¡¤ë°± ì™„ë£Œ: '{original_filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ DB ë³€ê²½ì‚¬í•­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(1)

    except Exception as e:
        print("\nâŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ!")
        traceback.print_exc()
        if conn:
            conn.rollback()
        sys.exit(1)
    finally:
        if conn:
            conn.close()
            print("\nğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("ğŸ ëª¨ë“  ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()