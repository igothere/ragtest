# rag.py

import os
import sys
import psycopg2
import traceback
import re # ì •ê·œ í‘œí˜„ì‹ì„ ìœ„í•´ ì¶”ê°€
from sentence_transformers import SentenceTransformer
from pdfminer.high_level import extract_text
from hlm_processors import process_file_to_hlm_chunks

# ... (ì„¤ì • ë¶€ë¶„ì€ ë™ì¼) ...
TARGET_DIR = "/home/eden/rag/docs"
MODEL_NAME = "nlpai-lab/KURE-v1"
DB_CONFIG = {
    "host": "localhost", "port": "5432", "dbname": "ragtest",
    "user": "eden", "password": "qwer123"
}
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100

# -------------------------------------------------
# í…ìŠ¤íŠ¸ ì •ê·œí™” í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
# -------------------------------------------------
def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”ë¥¼ ìœ„í•œ í•¨ìˆ˜"""
    # 1. ì†Œë¬¸ìë¡œ ë³€í™˜
    text = text.lower()
    
    # 2. ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ êµ¬ë‘ì  ì œì™¸)
    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ê·¸ë¦¬ê³  ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•œ ì˜¨ì (.)ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    # text = re.sub(r'[^ê°€-í£a-z0-9\s\.]', ' ', text)
    text = re.sub(r'[^ê°€-í£a-z0-9\s\./_-]', ' ', text)
    
    # 3. ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¶•ì†Œ
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 4. (ì„ íƒì‚¬í•­) ë¶ˆìš©ì–´(Stopword) ì œê±°
    # ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•Šê±°ë‚˜, ì˜¤íˆë ¤ ë¬¸ë§¥ì„ í•´ì¹  ìˆ˜ ìˆì–´ ìµœê·¼ì—ëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•Šê¸°ë„ í•©ë‹ˆë‹¤.
    # í•„ìš” ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³ , ë¶ˆìš©ì–´ ì‚¬ì „ì„ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
    # stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ê³¼', 'ì™€', 'ë„']
    # words = text.split()
    # words = [word for word in words if word not in stopwords]
    # text = ' '.join(words)
    
    return text
# -------------------------------------------------

# ... (chunk_text í•¨ìˆ˜ëŠ” ë™ì¼) ...
def chunk_text(text, chunk_size, overlap):
    if not text: return []
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks

# DB ì‚½ì… í•¨ìˆ˜ ìˆ˜ì •: original_filename ì¸ì ì¶”ê°€
def insert_chunk_to_db(cursor, title, content, embedding, source_file, original_filename, chunk_index, total_chunks):
    embedding_str = "[" + ",".join(map(str, embedding)) + "]"
    sql = """
        INSERT INTO documents (title, content, embedding, source_file, original_filename, chunk_index, total_chunks)
        VALUES (%s, %s, %s::vector, %s, %s, %s, %s)
    """
    try:
        print(f"    ğŸ’¾ DB ì‚½ì… ì‹œë„: ì²­í¬ {chunk_index}/{total_chunks}")
        cursor.execute(sql, (title, content, embedding_str, source_file, original_filename, chunk_index, total_chunks))
        print(f"    âœ… DB ì‚½ì… ì„±ê³µ")
    except Exception as e:
        print(f"    âŒ DB ì‚½ì… ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ ìˆ˜ì •: original_filename ì¸ì ì¶”ê°€
def process_file(file_path, unique_filename, original_filename, model, cursor):
    """ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ ë‹¨ìœ„ë¡œ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\nğŸ“„ '{original_filename}' (ì €ì¥ëª…: {unique_filename}) íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
    try:
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‚´ìš© ì¶”ì¶œ
        # ext = os.path.splitext(unique_filename)[1].lower()
        text = ""
        # if ext == '.pdf':
        #     text = extract_text(file_path)
        # elif ext in ['.txt', '.md']:
        #     with open(file_path, 'r', encoding='utf-8') as f:
        #         text = f.read()
        # else:
        #     print(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {unique_filename}")
        #     return False

        # if not text or not text.strip():
        #     print(f"  âš ï¸ ë‚´ìš©ì´ ì—†ëŠ” íŒŒì¼ì…ë‹ˆë‹¤: {original_filename}")
        #     return True
        print("ğŸ§  HLM êµ¬ì¡° ë¶„ì„ + ì²­í‚¹ ìˆ˜í–‰ ì¤‘...")
        text = process_file_to_hlm_chunks(file_path)
        total_chunks = len(text)
        print(f"  âœ‚ï¸ {total_chunks}ê°œì˜ HLM ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

        if total_chunks == 0:
            print(f"  âš ï¸ ë‚´ìš©ì´ ì—†ëŠ” íŒŒì¼ì…ë‹ˆë‹¤: {original_filename}")
            return True

        # --- ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ---
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•˜ê¸° ì „ì— ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        print("  - í…ìŠ¤íŠ¸ ì •ê·œí™” ìˆ˜í–‰ ì¤‘...")
        # normalized_text = normalize_text(text)
        # print(f"  - ì •ê·œí™” ì™„ë£Œ (ì›ë³¸: {len(text)}ì -> ì •ê·œí™”: {len(normalized_text)}ì)")
        # ---------------------------

        # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹
        chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)
        total_chunks = len(chunks)
        print(f"  âœ‚ï¸ {total_chunks}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

        # ê° ì²­í¬ë¥¼ ì„ë² ë”©í•˜ê³  DBì— ì €ì¥
        for i, chunk in enumerate(chunks):
            # ì²­í¬ ìì²´ëŠ” ì´ë¯¸ ì •ê·œí™”ë˜ì—ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ì„ë² ë”©
            if not chunk: continue # ë¹ˆ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
            
            chunk_index = i + 1
            print(f"  ğŸ”„ ì²­í¬ {chunk_index}/{total_chunks} ì²˜ë¦¬ ì¤‘...")
            
            embedding = model.encode(chunk).tolist()
            
            chunk_title = f"{original_filename}_chunk_{chunk_index}"
            insert_chunk_to_db(cursor, chunk["title"], chunk["content"], embedding, unique_filename, original_filename, chunk_index, total_chunks)

        print(f"  âœ… íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ: {original_filename}")
        return True

    except Exception as e:
        print(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {original_filename} - {e}")
        traceback.print_exc()
        return False

def main():
    print("==============================================")
    print("ğŸš€ RAG ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰")
    print("==============================================")
    
    # ì¸ì 2ê°œë¥¼ ë°›ë„ë¡ ìˆ˜ì •
    if len(sys.argv) < 3:
        print("âŒ ì‚¬ìš©ë²•: python rag.py <ê³ ìœ  íŒŒì¼ëª…> <ì›ë³¸ íŒŒì¼ëª…>")
        sys.exit(1)
    
    unique_filename = sys.argv[1]
    original_filename = sys.argv[2]
    print(f"ğŸ¯ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼: {original_filename} (ê³ ìœ ëª…: {unique_filename})")

    model = None
    conn = None
    try:
        print("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        model = SentenceTransformer(MODEL_NAME)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        print("ğŸ˜ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„...")
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")

        # íŒŒì¼ ê²½ë¡œëŠ” ê³ ìœ  íŒŒì¼ëª…ìœ¼ë¡œ í™•ì¸
        file_path = os.path.join(TARGET_DIR, unique_filename)
        print(f"ğŸ” íŒŒì¼ ê²½ë¡œ í™•ì¸: {file_path}")
        if not os.path.isfile(file_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            sys.exit(1)

        # process_file í˜¸ì¶œ ì‹œ ë‘ íŒŒì¼ëª… ëª¨ë‘ ì „ë‹¬
        if process_file(file_path, unique_filename, original_filename, model, cur):
            conn.commit()
            print(f"\nğŸ‰ ìµœì¢… ì»¤ë°‹ ì™„ë£Œ: '{original_filename}' ë°ì´í„°ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            conn.rollback()
            print(f"\nâš ï¸ ë¡¤ë°± ì™„ë£Œ: '{original_filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ DB ë³€ê²½ì‚¬í•­ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            sys.exit(1)

    except Exception as e:
        print("\nâŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ!")
        traceback.print_exc()
        if conn:
            conn.rollback()
        sys.exit(1)
    finally:
        if conn:
            conn.close()
            print("\nğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    print("ğŸ ëª¨ë“  ì‘ì—…ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()