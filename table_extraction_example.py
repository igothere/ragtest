#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDF í‘œ ì¶”ì¶œ ë° êµ¬ì¡°í™”ëœ ì²­í‚¹ ì˜ˆì œ

í•„ìš”í•œ íŒ¨í‚¤ì§€:
pip install tabula-py camelot-py[cv] pdfplumber

ê° ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ íŠ¹ì§•:
- tabula-py: Java ê¸°ë°˜, ì •í™•ë„ ë†’ìŒ
- camelot: Python ë„¤ì´í‹°ë¸Œ, ì„¤ì • ì˜µì…˜ ë§ŽìŒ  
- pdfplumber: ê°€ë²¼ì›€, í…ìŠ¤íŠ¸ì™€ í‘œ ë™ì‹œ ì²˜ë¦¬ ê°€ëŠ¥
"""

import pandas as pd
import pdfplumber
import json
from typing import List, Dict, Any
import re

def extract_tables_with_pdfplumber(pdf_path: str) -> List[Dict[str, Any]]:
    """pdfplumberë¥¼ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ í‘œì™€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ"""
    results = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, 1):
            print(f"ðŸ“„ íŽ˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘...")
            
            # í‘œ ì¶”ì¶œ
            tables = page.extract_tables()
            
            if tables:
                for table_idx, table in enumerate(tables):
                    if table and len(table) > 1:  # í—¤ë” + ìµœì†Œ 1í–‰ ì´ìƒ
                        # í‘œë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                        df = pd.DataFrame(table[1:], columns=table[0])
                        
                        # ë¹ˆ í–‰/ì—´ ì œê±°
                        df = df.dropna(how='all').dropna(axis=1, how='all')
                        
                        if not df.empty:
                            table_info = {
                                'type': 'table',
                                'page': page_num,
                                'table_index': table_idx + 1,
                                'data': df.to_dict('records'),
                                'columns': df.columns.tolist(),
                                'markdown': df.to_markdown(index=False),
                                'csv': df.to_csv(index=False),
                                'summary': f"íŽ˜ì´ì§€ {page_num}ì˜ í‘œ {table_idx + 1}: {len(df)}í–‰ Ã— {len(df.columns)}ì—´"
                            }
                            results.append(table_info)
                            print(f"  âœ… í‘œ ë°œê²¬: {len(df)}í–‰ Ã— {len(df.columns)}ì—´")
            
            # í‘œê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = page.extract_text()
            if text and text.strip():
                # í‘œ ì˜ì—­ì„ ì œì™¸í•œ í…ìŠ¤íŠ¸ (ê·¼ì‚¬ì¹˜)
                text_info = {
                    'type': 'text',
                    'page': page_num,
                    'content': text.strip(),
                    'summary': f"íŽ˜ì´ì§€ {page_num}ì˜ í…ìŠ¤íŠ¸: {len(text)}ìž"
                }
                results.append(text_info)
                print(f"  ðŸ“ í…ìŠ¤íŠ¸: {len(text)}ìž")
    
    return results

def create_structured_chunks(extracted_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ì¶”ì¶œëœ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ ì²­í¬ë¡œ ë³€í™˜"""
    chunks = []
    
    for item in extracted_data:
        if item['type'] == 'table':
            # í‘œëŠ” ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ì²­í‚¹ ê°€ëŠ¥
            
            # ë°©ë²• 1: ì „ì²´ í‘œë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
            table_chunk = {
                'type': 'table',
                'content': item['markdown'],  # ë§ˆí¬ë‹¤ìš´ í˜•ì‹
                'metadata': {
                    'page': item['page'],
                    'table_index': item['table_index'],
                    'rows': len(item['data']),
                    'columns': len(item['columns']),
                    'column_names': item['columns']
                },
                'searchable_text': f"{item['summary']} ì»¬ëŸ¼: {', '.join(item['columns'])} " + 
                                 ' '.join([str(cell) for row in item['data'] for cell in row.values() if cell]),
                'structured_data': item['data']
            }
            chunks.append(table_chunk)
            
            # ë°©ë²• 2: í‘œê°€ í¬ë©´ í–‰ ë‹¨ìœ„ë¡œ ë¶„í• 
            if len(item['data']) > 10:  # 10í–‰ ì´ìƒì´ë©´ ë¶„í• 
                for i in range(0, len(item['data']), 5):  # 5í–‰ì”© ë¶„í• 
                    sub_data = item['data'][i:i+5]
                    sub_df = pd.DataFrame(sub_data)
                    
                    sub_chunk = {
                        'type': 'table_section',
                        'content': sub_df.to_markdown(index=False),
                        'metadata': {
                            'page': item['page'],
                            'table_index': item['table_index'],
                            'section': f"{i+1}-{min(i+5, len(item['data']))}í–‰",
                            'columns': item['columns']
                        },
                        'searchable_text': f"í‘œ ì„¹ì…˜ ({i+1}-{min(i+5, len(item['data']))}í–‰) " +
                                         ' '.join([str(cell) for row in sub_data for cell in row.values() if cell]),
                        'structured_data': sub_data
                    }
                    chunks.append(sub_chunk)
        
        elif item['type'] == 'text':
            # í…ìŠ¤íŠ¸ëŠ” ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²­í‚¹
            text_chunk = {
                'type': 'text',
                'content': item['content'],
                'metadata': {
                    'page': item['page']
                },
                'searchable_text': item['content']
            }
            chunks.append(text_chunk)
    
    return chunks

def format_chunk_for_embedding(chunk: Dict[str, Any]) -> str:
    """ì²­í¬ë¥¼ ìž„ë² ë”©ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if chunk['type'] in ['table', 'table_section']:
        # í‘œ ë°ì´í„°ëŠ” ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ + êµ¬ì¡° ì •ë³´ ì¡°í•©
        return f"[í‘œ ë°ì´í„°] {chunk['searchable_text']}\n\n{chunk['content']}"
    else:
        return chunk['searchable_text']

def main():
    pdf_path = "./docs/73eb14fc-da45-4b1e-a29f-c42d10500155.pdf"
    
    print("ðŸ” PDFì—ì„œ í‘œì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    extracted_data = extract_tables_with_pdfplumber(pdf_path)
    
    print(f"\nðŸ“Š ì¶”ì¶œ ê²°ê³¼:")
    tables_count = len([item for item in extracted_data if item['type'] == 'table'])
    text_count = len([item for item in extracted_data if item['type'] == 'text'])
    print(f"  - í‘œ: {tables_count}ê°œ")
    print(f"  - í…ìŠ¤íŠ¸ ì„¹ì…˜: {text_count}ê°œ")
    
    print("\nðŸ§© êµ¬ì¡°í™”ëœ ì²­í¬ ìƒì„± ì¤‘...")
    chunks = create_structured_chunks(extracted_data)
    
    print(f"\nâœ… ìµœì¢… ì²­í¬: {len(chunks)}ê°œ")
    for i, chunk in enumerate(chunks, 1):
        print(f"  {i}. {chunk['type']} - {len(chunk['content'])}ìž")
        if chunk['type'] in ['table', 'table_section']:
            print(f"     ë©”íƒ€ë°ì´í„°: {chunk['metadata']}")
    
    # ìž„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ
    print("\nðŸ“ ìž„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì˜ˆì‹œ:")
    for i, chunk in enumerate(chunks[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
        embedding_text = format_chunk_for_embedding(chunk)
        print(f"\n--- ì²­í¬ {i} ---")
        print(embedding_text[:200] + "..." if len(embedding_text) > 200 else embedding_text)

if __name__ == "__main__":
    main()